[2024-01-31 17:57:49 swin_tiny_patch4_window7_224_22k] (main.py 357): INFO Full config saved to output/swin_tiny_patch4_window7_224_22k/default/config.json
[2024-01-31 17:57:49 swin_tiny_patch4_window7_224_22k] (main.py 360): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: tiny-imagenet
  DATA_PATH: ''
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_22k/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 90
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 7.8125e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 7.8125e-09
  WEIGHT_DECAY: 0.05

[2024-01-31 17:57:49 swin_tiny_patch4_window7_224_22k] (main.py 361): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_22k.yaml", "opts": null, "batch_size": 32, "data_path": null, "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-01-31 17:57:52 swin_tiny_patch4_window7_224_22k] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224_22k
[2024-01-31 17:57:53 swin_tiny_patch4_window7_224_22k] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2024-01-31 17:57:53 swin_tiny_patch4_window7_224_22k] (main.py 98): INFO number of params: 27673154
[2024-01-31 17:57:53 swin_tiny_patch4_window7_224_22k] (main.py 101): INFO number of GFLOPs: 4.49379072
[2024-01-31 17:57:53 swin_tiny_patch4_window7_224_22k] (main.py 135): INFO no checkpoint found in output/swin_tiny_patch4_window7_224_22k/default, ignoring auto resume
[2024-01-31 17:57:53 swin_tiny_patch4_window7_224_22k] (main.py 153): INFO Start training
[2024-01-31 17:57:57 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][0/3125]	eta 3:40:03 lr 0.000000	 wd 0.0500	time 4.2252 (4.2252)	loss 5.4105 (5.4105)	grad_norm 8.0393 (8.0393)	loss_scale 65536.0000 (65536.0000)	mem 3069MB
[2024-01-31 17:57:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][10/3125]	eta 0:29:42 lr 0.000000	 wd 0.0500	time 0.1987 (0.5722)	loss 5.3903 (5.3688)	grad_norm 6.9600 (7.3444)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:01 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][20/3125]	eta 0:20:17 lr 0.000000	 wd 0.0500	time 0.1922 (0.3920)	loss 5.2589 (5.3575)	grad_norm 6.5689 (7.1220)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:03 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][30/3125]	eta 0:16:57 lr 0.000000	 wd 0.0500	time 0.2047 (0.3288)	loss 5.3424 (5.3662)	grad_norm 8.1927 (7.1250)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:05 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][40/3125]	eta 0:15:14 lr 0.000000	 wd 0.0500	time 0.1987 (0.2966)	loss 5.3245 (5.3618)	grad_norm 6.1575 (7.1114)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][50/3125]	eta 0:14:10 lr 0.000000	 wd 0.0500	time 0.1931 (0.2767)	loss 5.4490 (5.3698)	grad_norm 7.3733 (7.1081)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][60/3125]	eta 0:13:27 lr 0.000000	 wd 0.0500	time 0.1933 (0.2633)	loss 5.4079 (5.3694)	grad_norm 6.8053 (7.1298)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:11 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][70/3125]	eta 0:12:55 lr 0.000000	 wd 0.0500	time 0.1927 (0.2539)	loss 5.3793 (5.3754)	grad_norm 7.2400 (7.1419)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:13 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][80/3125]	eta 0:12:30 lr 0.000000	 wd 0.0500	time 0.1933 (0.2465)	loss 5.4004 (5.3760)	grad_norm 7.8808 (7.1116)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:15 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][90/3125]	eta 0:12:10 lr 0.000000	 wd 0.0500	time 0.1933 (0.2407)	loss 5.4374 (5.3780)	grad_norm 7.4821 (7.1618)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:17 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][100/3125]	eta 0:11:54 lr 0.000000	 wd 0.0500	time 0.1940 (0.2361)	loss 5.2801 (5.3775)	grad_norm 7.6977 (7.1588)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:19 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][110/3125]	eta 0:11:40 lr 0.000000	 wd 0.0500	time 0.1930 (0.2323)	loss 5.2882 (5.3761)	grad_norm 6.8159 (7.1638)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][120/3125]	eta 0:11:28 lr 0.000000	 wd 0.0500	time 0.1935 (0.2291)	loss 5.3797 (5.3755)	grad_norm 7.8049 (7.1553)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][130/3125]	eta 0:11:17 lr 0.000000	 wd 0.0500	time 0.1931 (0.2264)	loss 5.3762 (5.3740)	grad_norm 6.7632 (7.1421)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][140/3125]	eta 0:11:11 lr 0.000000	 wd 0.0500	time 0.1966 (0.2248)	loss 5.4092 (5.3743)	grad_norm 7.7241 (7.1341)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][150/3125]	eta 0:11:03 lr 0.000000	 wd 0.0500	time 0.1940 (0.2229)	loss 5.4259 (5.3738)	grad_norm 6.8821 (7.1076)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][160/3125]	eta 0:10:56 lr 0.000000	 wd 0.0500	time 0.2007 (0.2213)	loss 5.3480 (5.3744)	grad_norm 7.0417 (7.0918)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][170/3125]	eta 0:10:49 lr 0.000000	 wd 0.0500	time 0.1943 (0.2197)	loss 5.2990 (5.3727)	grad_norm 6.6689 (7.0863)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][180/3125]	eta 0:10:42 lr 0.000000	 wd 0.0500	time 0.1938 (0.2183)	loss 5.3492 (5.3709)	grad_norm 6.7937 (7.0969)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][190/3125]	eta 0:10:37 lr 0.000000	 wd 0.0500	time 0.1936 (0.2171)	loss 5.3640 (5.3715)	grad_norm 7.2170 (7.1028)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][200/3125]	eta 0:10:32 lr 0.000000	 wd 0.0500	time 0.1938 (0.2161)	loss 5.3888 (5.3720)	grad_norm 6.3371 (7.1079)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][210/3125]	eta 0:10:27 lr 0.000000	 wd 0.0500	time 0.1938 (0.2152)	loss 5.3560 (5.3729)	grad_norm 6.8844 (7.1003)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:40 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][220/3125]	eta 0:10:23 lr 0.000000	 wd 0.0500	time 0.2078 (0.2145)	loss 5.4310 (5.3715)	grad_norm 8.1384 (7.0990)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][230/3125]	eta 0:10:20 lr 0.000000	 wd 0.0500	time 0.2097 (0.2143)	loss 5.3748 (5.3704)	grad_norm 7.8601 (7.0952)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][240/3125]	eta 0:10:16 lr 0.000000	 wd 0.0500	time 0.1946 (0.2136)	loss 5.2853 (5.3701)	grad_norm 7.5398 (7.0990)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:46 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][250/3125]	eta 0:10:12 lr 0.000000	 wd 0.0500	time 0.1949 (0.2129)	loss 5.4841 (5.3689)	grad_norm 6.5398 (7.0991)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:48 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][260/3125]	eta 0:10:08 lr 0.000000	 wd 0.0500	time 0.1946 (0.2123)	loss 5.3553 (5.3704)	grad_norm 7.8808 (7.1023)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:50 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][270/3125]	eta 0:10:05 lr 0.000000	 wd 0.0500	time 0.2100 (0.2120)	loss 5.2525 (5.3698)	grad_norm 7.8004 (7.1274)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][280/3125]	eta 0:10:01 lr 0.000000	 wd 0.0500	time 0.1954 (0.2115)	loss 5.3505 (5.3691)	grad_norm 6.5237 (7.1117)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:54 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][290/3125]	eta 0:09:58 lr 0.000000	 wd 0.0500	time 0.1948 (0.2109)	loss 5.2774 (5.3688)	grad_norm 8.1718 (7.1142)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][300/3125]	eta 0:09:55 lr 0.000000	 wd 0.0500	time 0.2120 (0.2106)	loss 5.4234 (5.3692)	grad_norm 6.9110 (7.1217)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:58:58 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][310/3125]	eta 0:09:52 lr 0.000000	 wd 0.0500	time 0.2116 (0.2106)	loss 5.2822 (5.3678)	grad_norm 6.3781 (7.1201)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:00 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][320/3125]	eta 0:09:50 lr 0.000000	 wd 0.0500	time 0.2065 (0.2104)	loss 5.4113 (5.3679)	grad_norm 9.0016 (7.1126)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:02 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][330/3125]	eta 0:09:48 lr 0.000000	 wd 0.0500	time 0.2124 (0.2104)	loss 5.3169 (5.3673)	grad_norm 7.7889 (7.1113)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][340/3125]	eta 0:09:45 lr 0.000000	 wd 0.0500	time 0.1932 (0.2102)	loss 5.4370 (5.3668)	grad_norm 6.0119 (7.0928)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][350/3125]	eta 0:09:42 lr 0.000000	 wd 0.0500	time 0.2051 (0.2098)	loss 5.2365 (5.3661)	grad_norm 7.7119 (7.0941)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:08 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][360/3125]	eta 0:09:39 lr 0.000000	 wd 0.0500	time 0.1936 (0.2094)	loss 5.3622 (5.3653)	grad_norm 7.4702 (7.0955)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][370/3125]	eta 0:09:35 lr 0.000000	 wd 0.0500	time 0.1945 (0.2090)	loss 5.2482 (5.3652)	grad_norm 7.4085 (7.0938)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][380/3125]	eta 0:09:32 lr 0.000000	 wd 0.0500	time 0.2095 (0.2087)	loss 5.3593 (5.3653)	grad_norm 7.8035 (7.0973)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][390/3125]	eta 0:09:30 lr 0.000000	 wd 0.0500	time 0.2073 (0.2085)	loss 5.2782 (5.3644)	grad_norm 7.9731 (7.1050)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][400/3125]	eta 0:09:27 lr 0.000000	 wd 0.0500	time 0.1943 (0.2083)	loss 5.3188 (5.3643)	grad_norm 6.3685 (7.0987)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][410/3125]	eta 0:09:24 lr 0.000000	 wd 0.0500	time 0.1946 (0.2079)	loss 5.2854 (5.3642)	grad_norm 6.5497 (7.0948)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][420/3125]	eta 0:09:21 lr 0.000000	 wd 0.0500	time 0.1956 (0.2076)	loss 5.4379 (5.3640)	grad_norm 7.3938 (7.0981)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][430/3125]	eta 0:09:18 lr 0.000000	 wd 0.0500	time 0.1937 (0.2073)	loss 5.3646 (5.3638)	grad_norm 7.6568 (7.1020)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][440/3125]	eta 0:09:16 lr 0.000000	 wd 0.0500	time 0.1991 (0.2071)	loss 5.3739 (5.3632)	grad_norm 7.5630 (7.0968)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][450/3125]	eta 0:09:13 lr 0.000000	 wd 0.0500	time 0.2097 (0.2070)	loss 5.3581 (5.3621)	grad_norm 8.2953 (7.0965)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][460/3125]	eta 0:09:10 lr 0.000000	 wd 0.0500	time 0.1941 (0.2067)	loss 5.3053 (5.3619)	grad_norm 6.5946 (7.0998)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][470/3125]	eta 0:09:08 lr 0.000000	 wd 0.0500	time 0.1941 (0.2065)	loss 5.4076 (5.3620)	grad_norm 6.1185 (7.1032)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][480/3125]	eta 0:09:05 lr 0.000000	 wd 0.0500	time 0.1941 (0.2063)	loss 5.4018 (5.3617)	grad_norm 7.3692 (7.1018)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][490/3125]	eta 0:09:03 lr 0.000000	 wd 0.0500	time 0.1946 (0.2061)	loss 5.3195 (5.3618)	grad_norm 6.7266 (7.1013)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][500/3125]	eta 0:09:00 lr 0.000000	 wd 0.0500	time 0.1942 (0.2059)	loss 5.3243 (5.3615)	grad_norm 6.4686 (7.0967)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][510/3125]	eta 0:08:57 lr 0.000000	 wd 0.0500	time 0.1947 (0.2057)	loss 5.3408 (5.3609)	grad_norm 7.6431 (7.0878)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:40 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][520/3125]	eta 0:08:55 lr 0.000000	 wd 0.0500	time 0.1954 (0.2055)	loss 5.3732 (5.3605)	grad_norm 7.7680 (7.0881)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][530/3125]	eta 0:08:52 lr 0.000000	 wd 0.0500	time 0.1966 (0.2053)	loss 5.3369 (5.3603)	grad_norm 6.7663 (7.0969)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][540/3125]	eta 0:08:50 lr 0.000000	 wd 0.0500	time 0.1950 (0.2052)	loss 5.3940 (5.3600)	grad_norm 6.7617 (7.0977)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:46 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][550/3125]	eta 0:08:48 lr 0.000000	 wd 0.0500	time 0.2030 (0.2051)	loss 5.3533 (5.3596)	grad_norm 7.8208 (7.1057)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:48 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][560/3125]	eta 0:08:45 lr 0.000000	 wd 0.0500	time 0.1976 (0.2050)	loss 5.3789 (5.3592)	grad_norm 7.0046 (7.1042)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:50 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][570/3125]	eta 0:08:43 lr 0.000000	 wd 0.0500	time 0.2003 (0.2049)	loss 5.3434 (5.3588)	grad_norm 6.1366 (7.1010)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][580/3125]	eta 0:08:41 lr 0.000000	 wd 0.0500	time 0.1958 (0.2049)	loss 5.3538 (5.3591)	grad_norm 6.6479 (7.1044)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:54 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][590/3125]	eta 0:08:39 lr 0.000000	 wd 0.0500	time 0.1955 (0.2047)	loss 5.2764 (5.3587)	grad_norm 6.1475 (7.1000)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][600/3125]	eta 0:08:36 lr 0.000000	 wd 0.0500	time 0.1964 (0.2046)	loss 5.3493 (5.3590)	grad_norm 7.7545 (7.1024)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 17:59:58 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][610/3125]	eta 0:08:34 lr 0.000000	 wd 0.0500	time 0.1968 (0.2045)	loss 5.3334 (5.3588)	grad_norm 6.1447 (7.0986)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:00 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][620/3125]	eta 0:08:31 lr 0.000000	 wd 0.0500	time 0.2007 (0.2044)	loss 5.3621 (5.3586)	grad_norm 8.4474 (7.0982)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:02 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][630/3125]	eta 0:08:29 lr 0.000000	 wd 0.0500	time 0.2106 (0.2043)	loss 5.3539 (5.3588)	grad_norm 6.3047 (7.0989)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][640/3125]	eta 0:08:27 lr 0.000000	 wd 0.0500	time 0.1950 (0.2042)	loss 5.2263 (5.3582)	grad_norm 8.0149 (7.1014)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][650/3125]	eta 0:08:25 lr 0.000000	 wd 0.0500	time 0.1964 (0.2041)	loss 5.3460 (5.3576)	grad_norm 7.3324 (7.1015)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:08 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][660/3125]	eta 0:08:23 lr 0.000000	 wd 0.0500	time 0.2297 (0.2041)	loss 5.3065 (5.3577)	grad_norm 8.0677 (7.1068)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][670/3125]	eta 0:08:21 lr 0.000000	 wd 0.0500	time 0.1960 (0.2042)	loss 5.2610 (5.3574)	grad_norm 7.8192 (7.1060)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][680/3125]	eta 0:08:19 lr 0.000000	 wd 0.0500	time 0.1958 (0.2041)	loss 5.3537 (5.3578)	grad_norm 7.1070 (7.1110)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][690/3125]	eta 0:08:17 lr 0.000000	 wd 0.0500	time 0.2009 (0.2043)	loss 5.3608 (5.3578)	grad_norm 7.5244 (7.1147)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][700/3125]	eta 0:08:15 lr 0.000000	 wd 0.0500	time 0.1943 (0.2044)	loss 5.4153 (5.3572)	grad_norm 7.3994 (7.1150)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][710/3125]	eta 0:08:14 lr 0.000000	 wd 0.0500	time 0.1978 (0.2047)	loss 5.3323 (5.3570)	grad_norm 8.9540 (7.1191)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][720/3125]	eta 0:08:12 lr 0.000000	 wd 0.0500	time 0.2054 (0.2047)	loss 5.3441 (5.3562)	grad_norm 7.1358 (7.1236)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][730/3125]	eta 0:08:10 lr 0.000000	 wd 0.0500	time 0.1953 (0.2047)	loss 5.3949 (5.3558)	grad_norm 6.5633 (7.1285)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][740/3125]	eta 0:08:08 lr 0.000000	 wd 0.0500	time 0.1952 (0.2046)	loss 5.2817 (5.3556)	grad_norm 6.5182 (7.1225)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][750/3125]	eta 0:08:05 lr 0.000000	 wd 0.0500	time 0.1950 (0.2045)	loss 5.3768 (5.3554)	grad_norm 7.0681 (7.1240)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][760/3125]	eta 0:08:03 lr 0.000000	 wd 0.0500	time 0.2037 (0.2044)	loss 5.2913 (5.3550)	grad_norm 6.8940 (7.1262)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][770/3125]	eta 0:08:01 lr 0.000000	 wd 0.0500	time 0.1955 (0.2044)	loss 5.3422 (5.3546)	grad_norm 7.3205 (7.1259)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][780/3125]	eta 0:07:59 lr 0.000000	 wd 0.0500	time 0.1948 (0.2043)	loss 5.2383 (5.3543)	grad_norm 7.6814 (7.1281)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][790/3125]	eta 0:07:56 lr 0.000000	 wd 0.0500	time 0.1939 (0.2041)	loss 5.3052 (5.3542)	grad_norm 6.9484 (7.1279)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][800/3125]	eta 0:07:54 lr 0.000000	 wd 0.0500	time 0.2016 (0.2040)	loss 5.3469 (5.3541)	grad_norm 7.8288 (7.1254)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][810/3125]	eta 0:07:52 lr 0.000000	 wd 0.0500	time 0.1945 (0.2039)	loss 5.2947 (5.3539)	grad_norm 7.2312 (7.1266)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:40 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][820/3125]	eta 0:07:49 lr 0.000000	 wd 0.0500	time 0.1940 (0.2038)	loss 5.3368 (5.3536)	grad_norm 6.4724 (7.1235)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][830/3125]	eta 0:07:47 lr 0.000000	 wd 0.0500	time 0.1956 (0.2037)	loss 5.3920 (5.3532)	grad_norm 7.5296 (7.1287)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][840/3125]	eta 0:07:45 lr 0.000000	 wd 0.0500	time 0.1942 (0.2038)	loss 5.3076 (5.3530)	grad_norm 7.7460 (7.1347)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:46 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][850/3125]	eta 0:07:43 lr 0.000000	 wd 0.0500	time 0.2157 (0.2038)	loss 5.4016 (5.3527)	grad_norm 6.9519 (7.1401)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:48 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][860/3125]	eta 0:07:41 lr 0.000000	 wd 0.0500	time 0.1940 (0.2039)	loss 5.3005 (5.3522)	grad_norm 6.2192 (7.1390)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:50 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][870/3125]	eta 0:07:39 lr 0.000000	 wd 0.0500	time 0.1940 (0.2038)	loss 5.3182 (5.3517)	grad_norm 7.3627 (7.1410)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][880/3125]	eta 0:07:37 lr 0.000000	 wd 0.0500	time 0.2020 (0.2037)	loss 5.2874 (5.3515)	grad_norm 7.8061 (7.1451)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:54 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][890/3125]	eta 0:07:35 lr 0.000000	 wd 0.0500	time 0.1948 (0.2037)	loss 5.2741 (5.3511)	grad_norm 7.5636 (7.1456)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][900/3125]	eta 0:07:33 lr 0.000000	 wd 0.0500	time 0.1939 (0.2036)	loss 5.3340 (5.3509)	grad_norm 6.2121 (7.1475)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:00:58 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][910/3125]	eta 0:07:30 lr 0.000000	 wd 0.0500	time 0.1944 (0.2035)	loss 5.2295 (5.3505)	grad_norm 7.1590 (7.1435)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:00 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][920/3125]	eta 0:07:28 lr 0.000000	 wd 0.0500	time 0.2102 (0.2035)	loss 5.2154 (5.3500)	grad_norm 8.7685 (7.1461)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:02 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][930/3125]	eta 0:07:26 lr 0.000000	 wd 0.0500	time 0.1933 (0.2034)	loss 5.2020 (5.3497)	grad_norm 5.6271 (7.1412)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][940/3125]	eta 0:07:24 lr 0.000000	 wd 0.0500	time 0.1940 (0.2034)	loss 5.3814 (5.3496)	grad_norm 7.1016 (7.1434)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][950/3125]	eta 0:07:22 lr 0.000000	 wd 0.0500	time 0.1938 (0.2033)	loss 5.2783 (5.3494)	grad_norm 7.7035 (7.1477)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:08 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][960/3125]	eta 0:07:19 lr 0.000000	 wd 0.0500	time 0.1952 (0.2032)	loss 5.3107 (5.3493)	grad_norm 6.3953 (7.1476)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][970/3125]	eta 0:07:17 lr 0.000000	 wd 0.0500	time 0.1938 (0.2031)	loss 5.3078 (5.3491)	grad_norm 6.2469 (7.1415)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][980/3125]	eta 0:07:15 lr 0.000000	 wd 0.0500	time 0.1947 (0.2031)	loss 5.3161 (5.3490)	grad_norm 5.8386 (7.1418)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][990/3125]	eta 0:07:13 lr 0.000001	 wd 0.0500	time 0.1946 (0.2030)	loss 5.3443 (5.3487)	grad_norm 7.4279 (7.1435)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1000/3125]	eta 0:07:11 lr 0.000001	 wd 0.0500	time 0.1954 (0.2029)	loss 5.3131 (5.3484)	grad_norm 6.3999 (7.1398)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1010/3125]	eta 0:07:08 lr 0.000001	 wd 0.0500	time 0.1942 (0.2028)	loss 5.3446 (5.3481)	grad_norm 7.1906 (7.1359)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1020/3125]	eta 0:07:06 lr 0.000001	 wd 0.0500	time 0.1942 (0.2028)	loss 5.3500 (5.3478)	grad_norm 7.5613 (7.1392)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1030/3125]	eta 0:07:04 lr 0.000001	 wd 0.0500	time 0.1950 (0.2027)	loss 5.3632 (5.3478)	grad_norm 6.1593 (7.1373)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1040/3125]	eta 0:07:02 lr 0.000001	 wd 0.0500	time 0.1978 (0.2027)	loss 5.3579 (5.3475)	grad_norm 8.1567 (7.1394)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1050/3125]	eta 0:07:00 lr 0.000001	 wd 0.0500	time 0.1944 (0.2026)	loss 5.3115 (5.3473)	grad_norm 7.1275 (7.1422)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1060/3125]	eta 0:06:58 lr 0.000001	 wd 0.0500	time 0.1939 (0.2026)	loss 5.3611 (5.3468)	grad_norm 8.2956 (7.1418)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1070/3125]	eta 0:06:56 lr 0.000001	 wd 0.0500	time 0.1985 (0.2025)	loss 5.3243 (5.3468)	grad_norm 6.2741 (7.1431)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1080/3125]	eta 0:06:54 lr 0.000001	 wd 0.0500	time 0.1950 (0.2025)	loss 5.3016 (5.3465)	grad_norm 7.7167 (7.1416)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1090/3125]	eta 0:06:51 lr 0.000001	 wd 0.0500	time 0.1942 (0.2024)	loss 5.2722 (5.3461)	grad_norm 6.0250 (7.1395)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1100/3125]	eta 0:06:49 lr 0.000001	 wd 0.0500	time 0.1940 (0.2024)	loss 5.3191 (5.3458)	grad_norm 6.7271 (7.1399)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1110/3125]	eta 0:06:47 lr 0.000001	 wd 0.0500	time 0.1941 (0.2023)	loss 5.3453 (5.3458)	grad_norm 6.9046 (7.1367)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1120/3125]	eta 0:06:45 lr 0.000001	 wd 0.0500	time 0.1941 (0.2023)	loss 5.3637 (5.3455)	grad_norm 7.5158 (7.1345)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:41 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1130/3125]	eta 0:06:43 lr 0.000001	 wd 0.0500	time 0.1953 (0.2022)	loss 5.2929 (5.3453)	grad_norm 7.2637 (7.1326)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:43 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1140/3125]	eta 0:06:41 lr 0.000001	 wd 0.0500	time 0.1938 (0.2021)	loss 5.2873 (5.3452)	grad_norm 7.6388 (7.1309)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:45 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1150/3125]	eta 0:06:39 lr 0.000001	 wd 0.0500	time 0.1939 (0.2021)	loss 5.2824 (5.3449)	grad_norm 5.9942 (7.1294)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:47 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1160/3125]	eta 0:06:36 lr 0.000001	 wd 0.0500	time 0.1943 (0.2020)	loss 5.3762 (5.3450)	grad_norm 6.9859 (7.1309)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:49 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1170/3125]	eta 0:06:34 lr 0.000001	 wd 0.0500	time 0.1960 (0.2020)	loss 5.3212 (5.3447)	grad_norm 6.6863 (7.1298)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:51 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1180/3125]	eta 0:06:32 lr 0.000001	 wd 0.0500	time 0.1943 (0.2019)	loss 5.2185 (5.3443)	grad_norm 7.0774 (7.1278)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:53 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1190/3125]	eta 0:06:30 lr 0.000001	 wd 0.0500	time 0.1997 (0.2019)	loss 5.3050 (5.3442)	grad_norm 6.8526 (7.1249)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:55 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1200/3125]	eta 0:06:28 lr 0.000001	 wd 0.0500	time 0.1943 (0.2018)	loss 5.3931 (5.3441)	grad_norm 8.0432 (7.1236)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:57 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1210/3125]	eta 0:06:26 lr 0.000001	 wd 0.0500	time 0.1942 (0.2018)	loss 5.2657 (5.3441)	grad_norm 6.7573 (7.1250)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:01:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1220/3125]	eta 0:06:24 lr 0.000001	 wd 0.0500	time 0.1943 (0.2017)	loss 5.2712 (5.3438)	grad_norm 8.0998 (7.1275)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:01 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1230/3125]	eta 0:06:22 lr 0.000001	 wd 0.0500	time 0.1947 (0.2017)	loss 5.2698 (5.3437)	grad_norm 6.7275 (7.1248)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:03 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1240/3125]	eta 0:06:20 lr 0.000001	 wd 0.0500	time 0.1949 (0.2016)	loss 5.3046 (5.3435)	grad_norm 7.0115 (7.1223)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:05 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1250/3125]	eta 0:06:17 lr 0.000001	 wd 0.0500	time 0.2130 (0.2016)	loss 5.3657 (5.3433)	grad_norm 7.1981 (7.1210)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1260/3125]	eta 0:06:15 lr 0.000001	 wd 0.0500	time 0.1946 (0.2015)	loss 5.3078 (5.3431)	grad_norm 7.1378 (7.1190)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1270/3125]	eta 0:06:13 lr 0.000001	 wd 0.0500	time 0.1951 (0.2015)	loss 5.3267 (5.3429)	grad_norm 7.4189 (7.1178)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:11 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1280/3125]	eta 0:06:11 lr 0.000001	 wd 0.0500	time 0.1952 (0.2015)	loss 5.2918 (5.3426)	grad_norm 7.2484 (7.1196)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:13 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1290/3125]	eta 0:06:09 lr 0.000001	 wd 0.0500	time 0.1944 (0.2014)	loss 5.3394 (5.3424)	grad_norm 7.0854 (7.1164)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:15 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1300/3125]	eta 0:06:07 lr 0.000001	 wd 0.0500	time 0.1951 (0.2014)	loss 5.2877 (5.3421)	grad_norm 7.2360 (7.1167)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:17 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1310/3125]	eta 0:06:05 lr 0.000001	 wd 0.0500	time 0.1953 (0.2013)	loss 5.3552 (5.3420)	grad_norm 8.0573 (7.1195)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:19 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1320/3125]	eta 0:06:03 lr 0.000001	 wd 0.0500	time 0.1952 (0.2013)	loss 5.2948 (5.3416)	grad_norm 7.8929 (7.1179)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:21 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1330/3125]	eta 0:06:01 lr 0.000001	 wd 0.0500	time 0.1981 (0.2012)	loss 5.3191 (5.3413)	grad_norm 7.3986 (7.1178)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:23 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1340/3125]	eta 0:05:59 lr 0.000001	 wd 0.0500	time 0.1961 (0.2012)	loss 5.3073 (5.3411)	grad_norm 7.5421 (7.1157)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:25 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1350/3125]	eta 0:05:57 lr 0.000001	 wd 0.0500	time 0.1952 (0.2012)	loss 5.3336 (5.3409)	grad_norm 7.3038 (7.1152)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1360/3125]	eta 0:05:54 lr 0.000001	 wd 0.0500	time 0.1950 (0.2011)	loss 5.3248 (5.3407)	grad_norm 6.0376 (7.1127)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1370/3125]	eta 0:05:52 lr 0.000001	 wd 0.0500	time 0.1945 (0.2011)	loss 5.2929 (5.3405)	grad_norm 6.9511 (7.1109)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1380/3125]	eta 0:05:50 lr 0.000001	 wd 0.0500	time 0.1985 (0.2011)	loss 5.3347 (5.3403)	grad_norm 6.3161 (7.1077)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1390/3125]	eta 0:05:48 lr 0.000001	 wd 0.0500	time 0.1984 (0.2011)	loss 5.2852 (5.3400)	grad_norm 7.0626 (7.1078)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1400/3125]	eta 0:05:46 lr 0.000001	 wd 0.0500	time 0.1955 (0.2010)	loss 5.3219 (5.3397)	grad_norm 6.4297 (7.1066)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1410/3125]	eta 0:05:44 lr 0.000001	 wd 0.0500	time 0.2088 (0.2010)	loss 5.3426 (5.3395)	grad_norm 7.3457 (7.1055)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1420/3125]	eta 0:05:42 lr 0.000001	 wd 0.0500	time 0.1974 (0.2010)	loss 5.3194 (5.3392)	grad_norm 6.8655 (7.1039)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:40 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1430/3125]	eta 0:05:40 lr 0.000001	 wd 0.0500	time 0.1975 (0.2010)	loss 5.2669 (5.3391)	grad_norm 7.3399 (7.1040)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1440/3125]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.1949 (0.2010)	loss 5.2635 (5.3388)	grad_norm 6.8934 (7.1029)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-01-31 18:02:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1450/3125]	eta 0:05:36 lr 0.000001	 wd 0.0500	time 0.1960 (0.2009)	loss 5.3636 (5.3385)	grad_norm 7.2491 (7.1065)	loss_scale 65536.0000 (65536.0000)	mem 3393MB
[2024-02-02 18:15:12 swin_tiny_patch4_window7_224_22k] (main.py 357): INFO Full config saved to output/swin_tiny_patch4_window7_224_22k/default/config.json
[2024-02-02 18:15:12 swin_tiny_patch4_window7_224_22k] (main.py 360): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: tiny-imagenet
  DATA_PATH: ''
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_22k/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 90
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 7.8125e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 7.8125e-09
  WEIGHT_DECAY: 0.05

[2024-02-02 18:15:12 swin_tiny_patch4_window7_224_22k] (main.py 361): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_22k.yaml", "opts": null, "batch_size": 32, "data_path": null, "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-02-02 18:15:16 swin_tiny_patch4_window7_224_22k] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224_22k
[2024-02-02 18:15:16 swin_tiny_patch4_window7_224_22k] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2024-02-02 18:15:16 swin_tiny_patch4_window7_224_22k] (main.py 98): INFO number of params: 27673154
[2024-02-02 18:15:16 swin_tiny_patch4_window7_224_22k] (main.py 101): INFO number of GFLOPs: 4.49379072
[2024-02-02 18:29:02 swin_tiny_patch4_window7_224_22k] (main.py 357): INFO Full config saved to output/swin_tiny_patch4_window7_224_22k/default/config.json
[2024-02-02 18:29:02 swin_tiny_patch4_window7_224_22k] (main.py 360): INFO AMP_ENABLE: true
AMP_OPT_LEVEL: ''
AUG:
  AUTO_AUGMENT: rand-m9-mstd0.5-inc1
  COLOR_JITTER: 0.4
  CUTMIX: 1.0
  CUTMIX_MINMAX: null
  MIXUP: 0.8
  MIXUP_MODE: batch
  MIXUP_PROB: 1.0
  MIXUP_SWITCH_PROB: 0.5
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.25
BASE:
- ''
DATA:
  BATCH_SIZE: 32
  CACHE_MODE: part
  DATASET: tiny-imagenet
  DATA_PATH: ''
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  MASK_PATCH_SIZE: 32
  MASK_RATIO: 0.6
  NUM_WORKERS: 8
  PIN_MEMORY: true
  ZIP_MODE: false
ENABLE_AMP: false
EVAL_MODE: false
FUSED_LAYERNORM: false
FUSED_WINDOW_PROCESS: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.1
  DROP_RATE: 0.0
  LABEL_SMOOTHING: 0.1
  NAME: swin_tiny_patch4_window7_224_22k
  NUM_CLASSES: 1000
  PRETRAINED: ''
  RESUME: ''
  SIMMIM:
    NORM_TARGET:
      ENABLE: false
      PATCH_SIZE: 47
  SWIN:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    QKV_BIAS: true
    QK_SCALE: null
    WINDOW_SIZE: 7
  SWINV2:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    WINDOW_SIZE: 7
  SWIN_MLP:
    APE: false
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    IN_CHANS: 3
    MLP_RATIO: 4.0
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    PATCH_NORM: true
    PATCH_SIZE: 4
    WINDOW_SIZE: 7
  SWIN_MOE:
    APE: false
    AUX_LOSS_WEIGHT: 0.01
    CAPACITY_FACTOR: 1.25
    COSINE_ROUTER: false
    COSINE_ROUTER_DIM: 256
    COSINE_ROUTER_INIT_T: 0.5
    DEPTHS:
    - 2
    - 2
    - 6
    - 2
    EMBED_DIM: 96
    GATE_NOISE: 1.0
    INIT_STD: 0.02
    IN_CHANS: 3
    IS_GSHARD_LOSS: false
    MLP_FC2_BIAS: true
    MLP_RATIO: 4.0
    MOE_BLOCKS:
    - - -1
    - - -1
    - - -1
    - - -1
    MOE_DROP: 0.0
    NORMALIZE_GATE: false
    NUM_HEADS:
    - 3
    - 6
    - 12
    - 24
    NUM_LOCAL_EXPERTS: 1
    PATCH_NORM: true
    PATCH_SIZE: 4
    PRETRAINED_WINDOW_SIZES:
    - 0
    - 0
    - 0
    - 0
    QKV_BIAS: true
    QK_SCALE: null
    TOP_VALUE: 1
    USE_BPR: true
    WINDOW_SIZE: 7
  TYPE: swin
OUTPUT: output/swin_tiny_patch4_window7_224_22k/default
PRINT_FREQ: 10
SAVE_FREQ: 1
SEED: 0
TAG: default
TEST:
  CROP: true
  SEQUENTIAL: false
  SHUFFLE: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 7.8125e-06
  CLIP_GRAD: 5.0
  EPOCHS: 90
  LAYER_DECAY: 1.0
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    GAMMA: 0.1
    MULTISTEPS: []
    NAME: cosine
    WARMUP_PREFIX: true
  MIN_LR: 7.8125e-08
  MOE:
    SAVE_MASTER: false
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    EPS: 1.0e-08
    MOMENTUM: 0.9
    NAME: adamw
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 5
  WARMUP_LR: 7.8125e-09
  WEIGHT_DECAY: 0.05

[2024-02-02 18:29:02 swin_tiny_patch4_window7_224_22k] (main.py 361): INFO {"cfg": "configs/swin/swin_tiny_patch4_window7_224_22k.yaml", "opts": null, "batch_size": 32, "data_path": null, "zip": false, "cache_mode": "part", "pretrained": null, "resume": null, "accumulation_steps": null, "use_checkpoint": false, "disable_amp": false, "amp_opt_level": null, "output": "output", "tag": null, "eval": false, "throughput": false, "local_rank": 0, "fused_window_process": false, "fused_layernorm": false, "optim": null}
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 93): INFO Creating model:swin/swin_tiny_patch4_window7_224_22k
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 95): INFO SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=96, input_resolution=(56, 56), num_heads=3, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=96, window_size=(7, 7), num_heads=3
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.009)
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.018)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=192, input_resolution=(28, 28), num_heads=6, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=192, window_size=(7, 7), num_heads=6
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.027)
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.036)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.045)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.055)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.064)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.073)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          dim=384, input_resolution=(14, 14), num_heads=12, window_size=7, shift_size=3, mlp_ratio=4.0
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=384, window_size=(7, 7), num_heads=12
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.082)
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.091)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          dim=768, input_resolution=(7, 7), num_heads=24, window_size=7, shift_size=0, mlp_ratio=4.0
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            dim=768, window_size=(7, 7), num_heads=24
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath(drop_prob=0.100)
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=200, bias=True)
)
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 98): INFO number of params: 27673154
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 101): INFO number of GFLOPs: 4.49379072
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 135): INFO no checkpoint found in output/swin_tiny_patch4_window7_224_22k/default, ignoring auto resume
[2024-02-02 18:29:04 swin_tiny_patch4_window7_224_22k] (main.py 153): INFO Start training
[2024-02-02 18:29:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][0/3125]	eta 2:15:54 lr 0.000000	 wd 0.0500	time 2.6095 (2.6095)	loss 5.3822 (5.3822)	grad_norm 7.2968 (7.2968)	loss_scale 65536.0000 (65536.0000)	mem 2436MB
[2024-02-02 18:29:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][10/3125]	eta 0:19:38 lr 0.000000	 wd 0.0500	time 0.1598 (0.3782)	loss 5.4515 (5.3546)	grad_norm 7.2628 (7.3047)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][20/3125]	eta 0:14:03 lr 0.000000	 wd 0.0500	time 0.1514 (0.2715)	loss 5.3131 (5.3521)	grad_norm 6.0927 (7.1480)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][30/3125]	eta 0:12:03 lr 0.000000	 wd 0.0500	time 0.1513 (0.2337)	loss 5.3525 (5.3612)	grad_norm 7.4163 (7.1160)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:13 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][40/3125]	eta 0:11:01 lr 0.000000	 wd 0.0500	time 0.1580 (0.2143)	loss 5.3460 (5.3533)	grad_norm 6.4533 (7.1121)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:15 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][50/3125]	eta 0:10:22 lr 0.000000	 wd 0.0500	time 0.1508 (0.2024)	loss 5.4073 (5.3650)	grad_norm 7.3085 (7.1115)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][60/3125]	eta 0:09:56 lr 0.000000	 wd 0.0500	time 0.1639 (0.1945)	loss 5.4348 (5.3643)	grad_norm 6.0934 (7.1261)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][70/3125]	eta 0:09:37 lr 0.000000	 wd 0.0500	time 0.1524 (0.1889)	loss 5.4337 (5.3696)	grad_norm 7.1399 (7.1461)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:19 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][80/3125]	eta 0:09:24 lr 0.000000	 wd 0.0500	time 0.1594 (0.1855)	loss 5.3113 (5.3677)	grad_norm 7.5696 (7.1202)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:21 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][90/3125]	eta 0:09:16 lr 0.000000	 wd 0.0500	time 0.1512 (0.1834)	loss 5.3491 (5.3693)	grad_norm 7.2981 (7.1239)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:23 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][100/3125]	eta 0:09:11 lr 0.000000	 wd 0.0500	time 0.1714 (0.1822)	loss 5.4210 (5.3704)	grad_norm 7.0941 (7.1183)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:25 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][110/3125]	eta 0:09:07 lr 0.000000	 wd 0.0500	time 0.1844 (0.1815)	loss 5.4017 (5.3690)	grad_norm 6.9344 (7.1180)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][120/3125]	eta 0:09:00 lr 0.000000	 wd 0.0500	time 0.1635 (0.1800)	loss 5.3344 (5.3695)	grad_norm 7.5793 (7.1074)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][130/3125]	eta 0:08:57 lr 0.000000	 wd 0.0500	time 0.1594 (0.1796)	loss 5.3229 (5.3687)	grad_norm 6.4338 (7.1113)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][140/3125]	eta 0:08:52 lr 0.000000	 wd 0.0500	time 0.1897 (0.1785)	loss 5.3440 (5.3682)	grad_norm 7.3207 (7.1304)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:31 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][150/3125]	eta 0:08:49 lr 0.000000	 wd 0.0500	time 0.1538 (0.1781)	loss 5.4009 (5.3678)	grad_norm 7.4141 (7.1228)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:33 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][160/3125]	eta 0:08:43 lr 0.000000	 wd 0.0500	time 0.1587 (0.1767)	loss 5.3486 (5.3686)	grad_norm 6.4518 (7.1116)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][170/3125]	eta 0:08:38 lr 0.000000	 wd 0.0500	time 0.1531 (0.1756)	loss 5.3027 (5.3682)	grad_norm 6.8872 (7.1040)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][180/3125]	eta 0:08:34 lr 0.000000	 wd 0.0500	time 0.1605 (0.1746)	loss 5.3580 (5.3675)	grad_norm 6.4940 (7.1153)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][190/3125]	eta 0:08:29 lr 0.000000	 wd 0.0500	time 0.1527 (0.1738)	loss 5.3346 (5.3675)	grad_norm 6.9175 (7.1129)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][200/3125]	eta 0:08:26 lr 0.000000	 wd 0.0500	time 0.1527 (0.1731)	loss 5.4875 (5.3692)	grad_norm 6.7485 (7.1218)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:41 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][210/3125]	eta 0:08:23 lr 0.000000	 wd 0.0500	time 0.1680 (0.1726)	loss 5.3394 (5.3703)	grad_norm 7.4196 (7.1368)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][220/3125]	eta 0:08:20 lr 0.000000	 wd 0.0500	time 0.1600 (0.1724)	loss 5.4687 (5.3704)	grad_norm 7.9780 (7.1404)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][230/3125]	eta 0:08:17 lr 0.000000	 wd 0.0500	time 0.1542 (0.1719)	loss 5.4104 (5.3696)	grad_norm 7.7645 (7.1410)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:46 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][240/3125]	eta 0:08:14 lr 0.000000	 wd 0.0500	time 0.1694 (0.1713)	loss 5.3870 (5.3693)	grad_norm 6.5434 (7.1484)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:47 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][250/3125]	eta 0:08:10 lr 0.000000	 wd 0.0500	time 0.1661 (0.1707)	loss 5.4689 (5.3682)	grad_norm 7.1913 (7.1552)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:49 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][260/3125]	eta 0:08:09 lr 0.000000	 wd 0.0500	time 0.1789 (0.1710)	loss 5.2784 (5.3685)	grad_norm 7.1430 (7.1582)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:51 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][270/3125]	eta 0:08:08 lr 0.000000	 wd 0.0500	time 0.1928 (0.1710)	loss 5.3398 (5.3683)	grad_norm 7.1717 (7.1677)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][280/3125]	eta 0:08:05 lr 0.000000	 wd 0.0500	time 0.1678 (0.1707)	loss 5.3208 (5.3668)	grad_norm 6.3466 (7.1467)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:54 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][290/3125]	eta 0:08:03 lr 0.000000	 wd 0.0500	time 0.1702 (0.1706)	loss 5.3085 (5.3657)	grad_norm 7.5841 (7.1489)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][300/3125]	eta 0:08:00 lr 0.000000	 wd 0.0500	time 0.1522 (0.1700)	loss 5.3483 (5.3656)	grad_norm 6.6019 (7.1474)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:57 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][310/3125]	eta 0:07:57 lr 0.000000	 wd 0.0500	time 0.1548 (0.1697)	loss 5.2382 (5.3648)	grad_norm 6.4194 (7.1400)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:29:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][320/3125]	eta 0:07:54 lr 0.000000	 wd 0.0500	time 0.1567 (0.1693)	loss 5.4020 (5.3660)	grad_norm 7.2082 (7.1335)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:00 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][330/3125]	eta 0:07:52 lr 0.000000	 wd 0.0500	time 0.1572 (0.1689)	loss 5.3674 (5.3660)	grad_norm 8.0635 (7.1362)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:02 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][340/3125]	eta 0:07:49 lr 0.000000	 wd 0.0500	time 0.1587 (0.1685)	loss 5.3534 (5.3657)	grad_norm 5.8946 (7.1234)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:03 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][350/3125]	eta 0:07:46 lr 0.000000	 wd 0.0500	time 0.1526 (0.1681)	loss 5.2285 (5.3649)	grad_norm 8.1385 (7.1336)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:05 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][360/3125]	eta 0:07:43 lr 0.000000	 wd 0.0500	time 0.1767 (0.1678)	loss 5.3801 (5.3654)	grad_norm 8.4565 (7.1510)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][370/3125]	eta 0:07:41 lr 0.000000	 wd 0.0500	time 0.1640 (0.1676)	loss 5.2624 (5.3647)	grad_norm 7.4703 (7.1514)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:08 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][380/3125]	eta 0:07:39 lr 0.000000	 wd 0.0500	time 0.1649 (0.1674)	loss 5.2709 (5.3641)	grad_norm 7.0494 (7.1557)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][390/3125]	eta 0:07:37 lr 0.000000	 wd 0.0500	time 0.1812 (0.1672)	loss 5.3885 (5.3635)	grad_norm 6.7134 (7.1623)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:11 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][400/3125]	eta 0:07:35 lr 0.000000	 wd 0.0500	time 0.1542 (0.1670)	loss 5.3230 (5.3629)	grad_norm 6.2349 (7.1563)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:13 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][410/3125]	eta 0:07:32 lr 0.000000	 wd 0.0500	time 0.1641 (0.1667)	loss 5.2813 (5.3625)	grad_norm 6.9277 (7.1552)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][420/3125]	eta 0:07:30 lr 0.000000	 wd 0.0500	time 0.1538 (0.1664)	loss 5.5021 (5.3630)	grad_norm 8.3021 (7.1619)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][430/3125]	eta 0:07:27 lr 0.000000	 wd 0.0500	time 0.1568 (0.1662)	loss 5.2764 (5.3628)	grad_norm 7.8895 (7.1661)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][440/3125]	eta 0:07:25 lr 0.000000	 wd 0.0500	time 0.1589 (0.1659)	loss 5.3645 (5.3625)	grad_norm 8.0329 (7.1638)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:19 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][450/3125]	eta 0:07:23 lr 0.000000	 wd 0.0500	time 0.1521 (0.1657)	loss 5.3951 (5.3616)	grad_norm 7.7987 (7.1631)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:21 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][460/3125]	eta 0:07:20 lr 0.000000	 wd 0.0500	time 0.1607 (0.1654)	loss 5.3955 (5.3616)	grad_norm 6.5191 (7.1698)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][470/3125]	eta 0:07:18 lr 0.000000	 wd 0.0500	time 0.1624 (0.1652)	loss 5.4131 (5.3622)	grad_norm 5.8449 (7.1764)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][480/3125]	eta 0:07:16 lr 0.000000	 wd 0.0500	time 0.1528 (0.1649)	loss 5.3175 (5.3616)	grad_norm 7.3441 (7.1766)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:25 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][490/3125]	eta 0:07:13 lr 0.000000	 wd 0.0500	time 0.1537 (0.1647)	loss 5.2853 (5.3612)	grad_norm 6.9740 (7.1729)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:27 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][500/3125]	eta 0:07:11 lr 0.000000	 wd 0.0500	time 0.1529 (0.1645)	loss 5.3793 (5.3611)	grad_norm 6.6847 (7.1753)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][510/3125]	eta 0:07:09 lr 0.000000	 wd 0.0500	time 0.1577 (0.1643)	loss 5.2999 (5.3605)	grad_norm 7.1636 (7.1702)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][520/3125]	eta 0:07:07 lr 0.000000	 wd 0.0500	time 0.1527 (0.1641)	loss 5.4110 (5.3604)	grad_norm 7.6346 (7.1693)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:31 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][530/3125]	eta 0:07:05 lr 0.000000	 wd 0.0500	time 0.1572 (0.1639)	loss 5.3618 (5.3600)	grad_norm 6.1394 (7.1713)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:33 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][540/3125]	eta 0:07:03 lr 0.000000	 wd 0.0500	time 0.1537 (0.1637)	loss 5.3229 (5.3596)	grad_norm 6.8783 (7.1697)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:35 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][550/3125]	eta 0:07:01 lr 0.000000	 wd 0.0500	time 0.1631 (0.1636)	loss 5.3892 (5.3598)	grad_norm 7.6817 (7.1769)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][560/3125]	eta 0:06:59 lr 0.000000	 wd 0.0500	time 0.1620 (0.1635)	loss 5.3909 (5.3595)	grad_norm 6.9496 (7.1777)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][570/3125]	eta 0:06:57 lr 0.000000	 wd 0.0500	time 0.1526 (0.1633)	loss 5.3227 (5.3594)	grad_norm 6.1514 (7.1704)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][580/3125]	eta 0:06:55 lr 0.000000	 wd 0.0500	time 0.1522 (0.1632)	loss 5.4116 (5.3598)	grad_norm 6.9618 (7.1719)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:41 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][590/3125]	eta 0:06:53 lr 0.000000	 wd 0.0500	time 0.1539 (0.1631)	loss 5.3315 (5.3594)	grad_norm 6.2700 (7.1714)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][600/3125]	eta 0:06:51 lr 0.000000	 wd 0.0500	time 0.1543 (0.1629)	loss 5.4144 (5.3595)	grad_norm 7.8919 (7.1740)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][610/3125]	eta 0:06:49 lr 0.000000	 wd 0.0500	time 0.1554 (0.1628)	loss 5.3006 (5.3595)	grad_norm 6.2592 (7.1707)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:45 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][620/3125]	eta 0:06:47 lr 0.000000	 wd 0.0500	time 0.1576 (0.1627)	loss 5.3141 (5.3592)	grad_norm 8.1046 (7.1733)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:47 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][630/3125]	eta 0:06:45 lr 0.000000	 wd 0.0500	time 0.1560 (0.1626)	loss 5.3294 (5.3593)	grad_norm 6.6604 (7.1746)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:49 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][640/3125]	eta 0:06:43 lr 0.000000	 wd 0.0500	time 0.1586 (0.1625)	loss 5.3302 (5.3590)	grad_norm 9.0788 (7.1782)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:50 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][650/3125]	eta 0:06:41 lr 0.000000	 wd 0.0500	time 0.1564 (0.1624)	loss 5.3820 (5.3590)	grad_norm 7.8450 (7.1807)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][660/3125]	eta 0:06:40 lr 0.000000	 wd 0.0500	time 0.1549 (0.1623)	loss 5.3273 (5.3589)	grad_norm 7.8883 (7.1873)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:53 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][670/3125]	eta 0:06:38 lr 0.000000	 wd 0.0500	time 0.1564 (0.1622)	loss 5.4510 (5.3593)	grad_norm 8.1131 (7.1864)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:55 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][680/3125]	eta 0:06:36 lr 0.000000	 wd 0.0500	time 0.1556 (0.1621)	loss 5.2846 (5.3589)	grad_norm 7.2091 (7.1878)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][690/3125]	eta 0:06:34 lr 0.000000	 wd 0.0500	time 0.1532 (0.1620)	loss 5.3582 (5.3589)	grad_norm 7.2097 (7.1880)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:58 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][700/3125]	eta 0:06:32 lr 0.000000	 wd 0.0500	time 0.1537 (0.1619)	loss 5.3969 (5.3583)	grad_norm 7.2828 (7.1865)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:30:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][710/3125]	eta 0:06:30 lr 0.000000	 wd 0.0500	time 0.1551 (0.1618)	loss 5.3505 (5.3582)	grad_norm 8.5658 (7.1876)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:01 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][720/3125]	eta 0:06:28 lr 0.000000	 wd 0.0500	time 0.1544 (0.1617)	loss 5.3317 (5.3577)	grad_norm 7.8260 (7.1898)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:03 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][730/3125]	eta 0:06:26 lr 0.000000	 wd 0.0500	time 0.1539 (0.1616)	loss 5.4374 (5.3574)	grad_norm 6.6727 (7.1894)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][740/3125]	eta 0:06:25 lr 0.000000	 wd 0.0500	time 0.1547 (0.1615)	loss 5.2956 (5.3573)	grad_norm 5.9577 (7.1826)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][750/3125]	eta 0:06:23 lr 0.000000	 wd 0.0500	time 0.1538 (0.1614)	loss 5.3609 (5.3572)	grad_norm 6.6401 (7.1843)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][760/3125]	eta 0:06:21 lr 0.000000	 wd 0.0500	time 0.1658 (0.1614)	loss 5.2882 (5.3568)	grad_norm 6.8370 (7.1848)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][770/3125]	eta 0:06:19 lr 0.000000	 wd 0.0500	time 0.1590 (0.1613)	loss 5.2762 (5.3565)	grad_norm 7.1273 (7.1841)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][780/3125]	eta 0:06:18 lr 0.000000	 wd 0.0500	time 0.1626 (0.1613)	loss 5.2669 (5.3561)	grad_norm 7.3833 (7.1833)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][790/3125]	eta 0:06:16 lr 0.000000	 wd 0.0500	time 0.1584 (0.1612)	loss 5.2732 (5.3558)	grad_norm 7.3317 (7.1817)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][800/3125]	eta 0:06:14 lr 0.000000	 wd 0.0500	time 0.1554 (0.1612)	loss 5.3519 (5.3557)	grad_norm 8.0261 (7.1800)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:15 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][810/3125]	eta 0:06:13 lr 0.000000	 wd 0.0500	time 0.1529 (0.1611)	loss 5.3625 (5.3553)	grad_norm 7.0557 (7.1817)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:17 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][820/3125]	eta 0:06:11 lr 0.000000	 wd 0.0500	time 0.1610 (0.1611)	loss 5.3940 (5.3551)	grad_norm 6.8641 (7.1810)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][830/3125]	eta 0:06:09 lr 0.000000	 wd 0.0500	time 0.1542 (0.1610)	loss 5.3975 (5.3550)	grad_norm 7.6712 (7.1825)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][840/3125]	eta 0:06:07 lr 0.000000	 wd 0.0500	time 0.1598 (0.1610)	loss 5.2761 (5.3547)	grad_norm 7.8702 (7.1882)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:21 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][850/3125]	eta 0:06:06 lr 0.000000	 wd 0.0500	time 0.1543 (0.1609)	loss 5.2681 (5.3541)	grad_norm 6.9249 (7.1918)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:23 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][860/3125]	eta 0:06:04 lr 0.000000	 wd 0.0500	time 0.1589 (0.1609)	loss 5.3444 (5.3537)	grad_norm 6.4367 (7.1918)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][870/3125]	eta 0:06:02 lr 0.000000	 wd 0.0500	time 0.1539 (0.1608)	loss 5.3818 (5.3534)	grad_norm 7.6320 (7.1905)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][880/3125]	eta 0:06:00 lr 0.000000	 wd 0.0500	time 0.1648 (0.1607)	loss 5.3153 (5.3530)	grad_norm 7.3832 (7.1938)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][890/3125]	eta 0:05:59 lr 0.000000	 wd 0.0500	time 0.1548 (0.1607)	loss 5.2865 (5.3525)	grad_norm 7.6613 (7.1962)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:29 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][900/3125]	eta 0:05:57 lr 0.000000	 wd 0.0500	time 0.1652 (0.1608)	loss 5.3484 (5.3523)	grad_norm 5.8514 (7.1944)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:31 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][910/3125]	eta 0:05:55 lr 0.000000	 wd 0.0500	time 0.1528 (0.1607)	loss 5.2182 (5.3518)	grad_norm 7.0568 (7.1935)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][920/3125]	eta 0:05:54 lr 0.000000	 wd 0.0500	time 0.1570 (0.1606)	loss 5.3190 (5.3514)	grad_norm 7.8284 (7.1955)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][930/3125]	eta 0:05:52 lr 0.000000	 wd 0.0500	time 0.1537 (0.1606)	loss 5.2438 (5.3510)	grad_norm 5.9824 (7.1918)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:35 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][940/3125]	eta 0:05:50 lr 0.000000	 wd 0.0500	time 0.1535 (0.1605)	loss 5.3742 (5.3506)	grad_norm 6.7091 (7.1942)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:37 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][950/3125]	eta 0:05:48 lr 0.000000	 wd 0.0500	time 0.1547 (0.1605)	loss 5.3602 (5.3506)	grad_norm 7.3715 (7.1959)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][960/3125]	eta 0:05:47 lr 0.000000	 wd 0.0500	time 0.1535 (0.1604)	loss 5.3212 (5.3505)	grad_norm 6.3924 (7.1975)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:40 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][970/3125]	eta 0:05:45 lr 0.000000	 wd 0.0500	time 0.1532 (0.1603)	loss 5.3222 (5.3503)	grad_norm 6.6546 (7.1932)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][980/3125]	eta 0:05:43 lr 0.000000	 wd 0.0500	time 0.1525 (0.1603)	loss 5.2760 (5.3501)	grad_norm 6.1600 (7.1933)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:43 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][990/3125]	eta 0:05:42 lr 0.000001	 wd 0.0500	time 0.1550 (0.1603)	loss 5.3134 (5.3497)	grad_norm 6.6783 (7.1959)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:45 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1000/3125]	eta 0:05:40 lr 0.000001	 wd 0.0500	time 0.1578 (0.1602)	loss 5.2860 (5.3494)	grad_norm 6.6310 (7.1927)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:46 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1010/3125]	eta 0:05:38 lr 0.000001	 wd 0.0500	time 0.1559 (0.1602)	loss 5.3144 (5.3490)	grad_norm 6.3275 (7.1931)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:48 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1020/3125]	eta 0:05:37 lr 0.000001	 wd 0.0500	time 0.1544 (0.1601)	loss 5.3486 (5.3490)	grad_norm 7.1608 (7.1967)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:49 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1030/3125]	eta 0:05:35 lr 0.000001	 wd 0.0500	time 0.1537 (0.1601)	loss 5.3942 (5.3488)	grad_norm 6.1330 (7.1953)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:51 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1040/3125]	eta 0:05:33 lr 0.000001	 wd 0.0500	time 0.1543 (0.1601)	loss 5.3317 (5.3485)	grad_norm 7.1051 (7.1969)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:53 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1050/3125]	eta 0:05:32 lr 0.000001	 wd 0.0500	time 0.1580 (0.1601)	loss 5.3256 (5.3483)	grad_norm 7.1775 (7.1965)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:54 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1060/3125]	eta 0:05:30 lr 0.000001	 wd 0.0500	time 0.1803 (0.1601)	loss 5.3308 (5.3478)	grad_norm 7.5713 (7.1929)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1070/3125]	eta 0:05:29 lr 0.000001	 wd 0.0500	time 0.1570 (0.1601)	loss 5.3055 (5.3476)	grad_norm 6.7721 (7.1944)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:57 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1080/3125]	eta 0:05:27 lr 0.000001	 wd 0.0500	time 0.1648 (0.1601)	loss 5.3512 (5.3472)	grad_norm 8.0415 (7.1931)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:31:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1090/3125]	eta 0:05:25 lr 0.000001	 wd 0.0500	time 0.1551 (0.1601)	loss 5.2946 (5.3471)	grad_norm 6.9261 (7.1929)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:01 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1100/3125]	eta 0:05:24 lr 0.000001	 wd 0.0500	time 0.1546 (0.1600)	loss 5.2949 (5.3468)	grad_norm 6.9002 (7.1923)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:02 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1110/3125]	eta 0:05:22 lr 0.000001	 wd 0.0500	time 0.1668 (0.1601)	loss 5.3725 (5.3465)	grad_norm 7.6997 (7.1905)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1120/3125]	eta 0:05:21 lr 0.000001	 wd 0.0500	time 0.1546 (0.1602)	loss 5.3342 (5.3461)	grad_norm 7.4458 (7.1884)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1130/3125]	eta 0:05:19 lr 0.000001	 wd 0.0500	time 0.1613 (0.1602)	loss 5.3024 (5.3458)	grad_norm 7.7807 (7.1877)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:07 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1140/3125]	eta 0:05:17 lr 0.000001	 wd 0.0500	time 0.1571 (0.1601)	loss 5.3282 (5.3455)	grad_norm 7.7103 (7.1874)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1150/3125]	eta 0:05:16 lr 0.000001	 wd 0.0500	time 0.1610 (0.1601)	loss 5.2604 (5.3452)	grad_norm 6.0447 (7.1849)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:10 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1160/3125]	eta 0:05:14 lr 0.000001	 wd 0.0500	time 0.1590 (0.1601)	loss 5.3474 (5.3450)	grad_norm 6.1972 (7.1859)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1170/3125]	eta 0:05:12 lr 0.000001	 wd 0.0500	time 0.1548 (0.1600)	loss 5.3047 (5.3447)	grad_norm 7.5394 (7.1879)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:13 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1180/3125]	eta 0:05:11 lr 0.000001	 wd 0.0500	time 0.1540 (0.1600)	loss 5.2991 (5.3444)	grad_norm 6.6575 (7.1858)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:15 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1190/3125]	eta 0:05:09 lr 0.000001	 wd 0.0500	time 0.1547 (0.1600)	loss 5.3528 (5.3443)	grad_norm 7.0354 (7.1835)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:17 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1200/3125]	eta 0:05:07 lr 0.000001	 wd 0.0500	time 0.1569 (0.1600)	loss 5.3446 (5.3444)	grad_norm 7.7667 (7.1833)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:18 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1210/3125]	eta 0:05:06 lr 0.000001	 wd 0.0500	time 0.1621 (0.1599)	loss 5.3242 (5.3444)	grad_norm 7.2473 (7.1837)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1220/3125]	eta 0:05:04 lr 0.000001	 wd 0.0500	time 0.1532 (0.1599)	loss 5.2597 (5.3441)	grad_norm 7.6852 (7.1858)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:21 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1230/3125]	eta 0:05:03 lr 0.000001	 wd 0.0500	time 0.1535 (0.1599)	loss 5.3150 (5.3438)	grad_norm 6.6400 (7.1826)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:23 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1240/3125]	eta 0:05:01 lr 0.000001	 wd 0.0500	time 0.1557 (0.1599)	loss 5.3183 (5.3435)	grad_norm 7.6970 (7.1834)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1250/3125]	eta 0:04:59 lr 0.000001	 wd 0.0500	time 0.1554 (0.1598)	loss 5.3235 (5.3434)	grad_norm 7.3526 (7.1834)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:26 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1260/3125]	eta 0:04:58 lr 0.000001	 wd 0.0500	time 0.1579 (0.1598)	loss 5.2714 (5.3431)	grad_norm 7.0612 (7.1813)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:27 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1270/3125]	eta 0:04:56 lr 0.000001	 wd 0.0500	time 0.1817 (0.1598)	loss 5.3053 (5.3428)	grad_norm 7.7606 (7.1802)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:29 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1280/3125]	eta 0:04:55 lr 0.000001	 wd 0.0500	time 0.1695 (0.1599)	loss 5.1674 (5.3424)	grad_norm 8.1860 (7.1814)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:31 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1290/3125]	eta 0:04:53 lr 0.000001	 wd 0.0500	time 0.1556 (0.1599)	loss 5.4304 (5.3422)	grad_norm 7.5360 (7.1790)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:32 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1300/3125]	eta 0:04:51 lr 0.000001	 wd 0.0500	time 0.1528 (0.1599)	loss 5.2753 (5.3420)	grad_norm 7.2622 (7.1781)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:34 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1310/3125]	eta 0:04:50 lr 0.000001	 wd 0.0500	time 0.1540 (0.1599)	loss 5.2912 (5.3418)	grad_norm 9.0964 (7.1816)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1320/3125]	eta 0:04:48 lr 0.000001	 wd 0.0500	time 0.1598 (0.1599)	loss 5.3099 (5.3414)	grad_norm 7.8638 (7.1823)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:37 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1330/3125]	eta 0:04:47 lr 0.000001	 wd 0.0500	time 0.1627 (0.1599)	loss 5.2730 (5.3411)	grad_norm 6.9080 (7.1822)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1340/3125]	eta 0:04:45 lr 0.000001	 wd 0.0500	time 0.1559 (0.1599)	loss 5.3945 (5.3408)	grad_norm 8.3509 (7.1821)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:41 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1350/3125]	eta 0:04:43 lr 0.000001	 wd 0.0500	time 0.1759 (0.1600)	loss 5.3816 (5.3406)	grad_norm 6.5707 (7.1805)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1360/3125]	eta 0:04:42 lr 0.000001	 wd 0.0500	time 0.1606 (0.1600)	loss 5.3693 (5.3405)	grad_norm 6.5770 (7.1812)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1370/3125]	eta 0:04:40 lr 0.000001	 wd 0.0500	time 0.1587 (0.1600)	loss 5.2588 (5.3402)	grad_norm 6.1801 (7.1778)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:45 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1380/3125]	eta 0:04:39 lr 0.000001	 wd 0.0500	time 0.1545 (0.1600)	loss 5.3510 (5.3401)	grad_norm 6.9684 (7.1758)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:47 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1390/3125]	eta 0:04:37 lr 0.000001	 wd 0.0500	time 0.1574 (0.1600)	loss 5.3533 (5.3399)	grad_norm 7.9341 (7.1764)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:48 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1400/3125]	eta 0:04:35 lr 0.000001	 wd 0.0500	time 0.1539 (0.1599)	loss 5.2726 (5.3397)	grad_norm 7.0973 (7.1756)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:50 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1410/3125]	eta 0:04:34 lr 0.000001	 wd 0.0500	time 0.1548 (0.1599)	loss 5.2742 (5.3394)	grad_norm 7.3785 (7.1747)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:52 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1420/3125]	eta 0:04:32 lr 0.000001	 wd 0.0500	time 0.1527 (0.1599)	loss 5.2657 (5.3391)	grad_norm 6.3730 (7.1723)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:53 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1430/3125]	eta 0:04:30 lr 0.000001	 wd 0.0500	time 0.1540 (0.1599)	loss 5.3077 (5.3388)	grad_norm 7.3574 (7.1715)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:55 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1440/3125]	eta 0:04:29 lr 0.000001	 wd 0.0500	time 0.1532 (0.1598)	loss 5.2730 (5.3384)	grad_norm 6.9851 (7.1697)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:56 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1450/3125]	eta 0:04:27 lr 0.000001	 wd 0.0500	time 0.1535 (0.1598)	loss 5.3647 (5.3382)	grad_norm 7.1635 (7.1710)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:58 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1460/3125]	eta 0:04:26 lr 0.000001	 wd 0.0500	time 0.1585 (0.1598)	loss 5.3146 (5.3379)	grad_norm 6.4384 (7.1717)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:32:59 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1470/3125]	eta 0:04:24 lr 0.000001	 wd 0.0500	time 0.1529 (0.1598)	loss 5.3812 (5.3377)	grad_norm 8.2196 (7.1713)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:01 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1480/3125]	eta 0:04:22 lr 0.000001	 wd 0.0500	time 0.1586 (0.1599)	loss 5.3549 (5.3375)	grad_norm 8.3192 (7.1731)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:03 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1490/3125]	eta 0:04:21 lr 0.000001	 wd 0.0500	time 0.1538 (0.1599)	loss 5.3300 (5.3372)	grad_norm 6.6283 (7.1757)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:04 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1500/3125]	eta 0:04:19 lr 0.000001	 wd 0.0500	time 0.1538 (0.1599)	loss 5.2557 (5.3369)	grad_norm 6.1622 (7.1755)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:06 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1510/3125]	eta 0:04:18 lr 0.000001	 wd 0.0500	time 0.1551 (0.1599)	loss 5.2991 (5.3366)	grad_norm 6.6727 (7.1737)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:08 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1520/3125]	eta 0:04:16 lr 0.000001	 wd 0.0500	time 0.1732 (0.1599)	loss 5.2846 (5.3364)	grad_norm 7.8085 (7.1726)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:09 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1530/3125]	eta 0:04:15 lr 0.000001	 wd 0.0500	time 0.1540 (0.1599)	loss 5.3020 (5.3363)	grad_norm 7.4782 (7.1745)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:11 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1540/3125]	eta 0:04:13 lr 0.000001	 wd 0.0500	time 0.1644 (0.1599)	loss 5.2842 (5.3360)	grad_norm 8.8965 (7.1729)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:12 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1550/3125]	eta 0:04:11 lr 0.000001	 wd 0.0500	time 0.1542 (0.1599)	loss 5.2959 (5.3357)	grad_norm 6.8940 (7.1703)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:14 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1560/3125]	eta 0:04:10 lr 0.000001	 wd 0.0500	time 0.1669 (0.1600)	loss 5.2463 (5.3354)	grad_norm 7.4650 (7.1706)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:16 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1570/3125]	eta 0:04:08 lr 0.000001	 wd 0.0500	time 0.1562 (0.1600)	loss 5.2262 (5.3351)	grad_norm 6.9698 (7.1711)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:17 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1580/3125]	eta 0:04:07 lr 0.000001	 wd 0.0500	time 0.1577 (0.1600)	loss 5.1951 (5.3348)	grad_norm 7.9757 (7.1720)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:19 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1590/3125]	eta 0:04:05 lr 0.000001	 wd 0.0500	time 0.1550 (0.1600)	loss 5.2470 (5.3345)	grad_norm 6.8265 (7.1728)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:20 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1600/3125]	eta 0:04:03 lr 0.000001	 wd 0.0500	time 0.1642 (0.1599)	loss 5.3469 (5.3343)	grad_norm 7.9582 (7.1708)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:22 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1610/3125]	eta 0:04:02 lr 0.000001	 wd 0.0500	time 0.1536 (0.1599)	loss 5.3061 (5.3340)	grad_norm 6.5197 (7.1684)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:24 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1620/3125]	eta 0:04:00 lr 0.000001	 wd 0.0500	time 0.1556 (0.1599)	loss 5.3730 (5.3338)	grad_norm 7.0962 (7.1694)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:25 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1630/3125]	eta 0:03:59 lr 0.000001	 wd 0.0500	time 0.1594 (0.1599)	loss 5.2988 (5.3336)	grad_norm 6.0707 (7.1673)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:27 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1640/3125]	eta 0:03:57 lr 0.000001	 wd 0.0500	time 0.1535 (0.1599)	loss 5.2774 (5.3333)	grad_norm 7.6538 (7.1653)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:28 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1650/3125]	eta 0:03:55 lr 0.000001	 wd 0.0500	time 0.1644 (0.1599)	loss 5.3044 (5.3330)	grad_norm 6.9222 (7.1659)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:30 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1660/3125]	eta 0:03:54 lr 0.000001	 wd 0.0500	time 0.1600 (0.1598)	loss 5.3334 (5.3327)	grad_norm 7.1677 (7.1649)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:31 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1670/3125]	eta 0:03:52 lr 0.000001	 wd 0.0500	time 0.1648 (0.1598)	loss 5.3065 (5.3323)	grad_norm 5.2854 (7.1645)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:33 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1680/3125]	eta 0:03:50 lr 0.000001	 wd 0.0500	time 0.1595 (0.1598)	loss 5.2907 (5.3320)	grad_norm 6.6713 (7.1645)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:35 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1690/3125]	eta 0:03:49 lr 0.000001	 wd 0.0500	time 0.1587 (0.1598)	loss 5.2329 (5.3318)	grad_norm 6.9263 (7.1640)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:36 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1700/3125]	eta 0:03:47 lr 0.000001	 wd 0.0500	time 0.1581 (0.1598)	loss 5.3031 (5.3315)	grad_norm 7.7371 (7.1631)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:38 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1710/3125]	eta 0:03:46 lr 0.000001	 wd 0.0500	time 0.1537 (0.1598)	loss 5.3884 (5.3314)	grad_norm 7.9593 (7.1633)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:39 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1720/3125]	eta 0:03:44 lr 0.000001	 wd 0.0500	time 0.1545 (0.1598)	loss 5.3062 (5.3314)	grad_norm 7.9486 (7.1630)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:41 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1730/3125]	eta 0:03:42 lr 0.000001	 wd 0.0500	time 0.1525 (0.1597)	loss 5.2987 (5.3311)	grad_norm 7.0566 (7.1624)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:42 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1740/3125]	eta 0:03:41 lr 0.000001	 wd 0.0500	time 0.1528 (0.1597)	loss 5.3256 (5.3310)	grad_norm 6.3528 (7.1609)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
[2024-02-02 18:33:44 swin_tiny_patch4_window7_224_22k] (main.py 223): INFO Train: [0/90][1750/3125]	eta 0:03:39 lr 0.000001	 wd 0.0500	time 0.1527 (0.1597)	loss 5.2746 (5.3307)	grad_norm 6.8765 (7.1614)	loss_scale 65536.0000 (65536.0000)	mem 2760MB
